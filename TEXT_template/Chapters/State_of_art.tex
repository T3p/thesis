\chapter{Safe Reinforcement Learning:\newline State of the Art}
The aim of this chapter is to present the state of the art in safe approaches to Reinforcement Learning, also known as conservative or monotonically improving methods. 

In Section \ref{sec:safe_def} we define the policy oscillation problem and provide the motivations behind safe methods. In Section \ref{sec:spi} we describe safe approaches to policy iteration. In Section \ref{sec:spg} we present existing safe policy gradient methods. Finally, in Section \ref{sec:other_safe} we mention other interesting safe approaches to \ac{RL}.

\section{Problem definition and motivation} \label{sec:safe_def}
\subsection{The policy oscillation problem}
Safe \ac{RL} algorithms address a problem known as policy oscillation, first studied in the context of approximate policy iteration and not yet fully understood (see for instance \cite{Bertsekas2011api}, \cite{wagner2011reinterpretation}). Policy oscillation affects approximate \ac{RL} algorithms and manifests as a non-monotonic performance $J_\mu(\pi)$ over learning iterations. In parametric methods, a consequence is that the convergence of the parameter vector to a stationary point may not correspond to an increasing performance, but to an oscillating one. Even when the trend of $J_\mu(\pi)$ is generally positive, large oscillations can be unacceptable for some applications.
Among the main recognized causes of policy oscillation are the entity of approximation errors and the magnitude of policy updates.
To explain the phenomenon in more detail, we restrict our analysis to policy gradient methods. However, the nature of the problem is fundamentally the same also for policy iteration methods, as shown by Wagner \cite{wagner2011reinterpretation}. For policy gradient methods, non-monotonic improvement means that in some cases $J_\mu(\vtheta+\alpha\gradApp{\vtheta}) \leq J_\mu(\vtheta)$. The two main causes are:
\begin{enumerate}
\item The magnitude of the updates, controlled by the step size $\alpha$. Large parameter updates may result in overshooting in policy space, producing oscillation around a local optimum.
\item The policy gradient estimation error. The variance of $\gradApp{\vtheta}$ affects the quality of the updates in terms of direction, and may even produce updates in non-improving directions.
\end{enumerate} 

\subsection{The need for safe methods}
Both of the policy oscillation causes that have been described pose a trade-off between monotonic improvement and speed of convergence. Using small step sizes results in more conservative policy updates, reducing performance oscillation. On the other hand, a small step size can heavily affect convergence speed. Similarly, computing more accurate policy gradient estimates typically requires a large number of samples. Even if the batch size does not affect directly the number of iterations to convergence, it can affect the actual learning time, especially when collecting sample trajectories is time-wise expensive. The resulting trade-off is similar to the well known exploration-exploitation dilemma, and is particularly evident when learning is performed on real systems. In a simulated task with no time limitations, neither convergence speed nor oscillation are major issues, unless convergence is compromised. Instead, in more realistic scenarios, oscillation itself may cause the following problems:
\begin{itemize}
\item Dangerous behavior: low performance peaks may correspond to large deviations from the initial policy. Even if the initial policy has been designed to be safe, such deviations may result in behavior able to damage the system.
\item Poor long-term performance: Consider the scenario in which learning is performed simultaneously with production, e.g.\ to automatically adjust an existing optimal policy to small changes in the environment, either in a real-time manner or by temporary stopping production. Oscillations in the per-episode performance are paid in term of long-term performance when they are not justified by a relevant enough overall policy improvement. This loss is similar to the concept of regret caused by exploration.
\item Selection of suboptimal policies: if the learning process has a fixed duration or must be stopped for some reason, oscillations may lead to the selection of a policy that is not the best seen so far, or is even worse than the initial policy. In highly stochastic environments it can be very difficult to manually reject such sub-par policies, and testing all the policies seen so far may be prohibitively expensive.
\end{itemize} 
From all these problems comes the necessity of policy updates that are monotonically increasing.

\section{Safe Policy Iteration}\label{sec:spi}
In policy iteration, policy oscillation can arise each time approximation is used either in the evaluation step or in the improvement step.
In this section we present the main safe approaches approximate to policy iteration, starting from the seminal work by Kakade and Langford \cite{kakade2002approximately}.

\subsection{Conservative Policy Iteration}\label{sec:CPI}
In \cite{kakade2002approximately}, Kakade and Langford address the policy oscillation problem by limiting the magnitude of the policy updates.
Let $\pi$ be the current policy and $\pi'$ a greedy policy w.r.t.\ the latest action-value estimate, i.e. :
\[
	\pi'(a \mid s) \in \arg\max\limits_{a \in \mathcal{A}} \hat{Q}^{\pi}(s,a)
\]
The traditional approximate policy update is:
\[
	\pi_{new}(a \mid s) = \pi'(a \mid s).
\]
Instead, Kakade and Langford proposed to update $\pi$ with a convex combination of the greedy and the current policy:
\begin{equation} \label{eq:cpi}
	\pi_{new}(a \mid s) = (1-\alpha)\pi(a \mid s) + \alpha\pi'(a \mid s),
\end{equation}
with step size $\alpha \in [0,1]$. When a step size $\alpha<1$ is used, this is more conservative than a pure greedy improvement, producing a new policy that is more similar to the original one. 
The authors were able to compute a lower bound on the performance improvement:
\[
	J_\mu(\pi_{new}) - J_\mu(\pi) \geq \frac{\alpha}{1-\gamma}\left(\mathbb{A}_{\pi,\mu}(\pi')
		- \frac{2\alpha\gamma\epsilon}{1-\gamma(1-\alpha)}\right),
\]
where $\epsilon = \max\limits_{s \in \mathcal{S}}|\expected{a}{\pi'}{A^\pi(s,a)}|$. While the positive term obviously depends on the advantage of the greedy policy, the negative one must be intended as a penalty for large policy updates. By maximizing this bound w.r.t.\ $\alpha$, a step size is obtained that guarantees positive performance improvement as long as the advantage is positive, without totally sacrificing the speed of convergence:
\begin{align}
&\alpha^* = \frac{(1-\gamma)\mathbb{A}_{\pi,\mu}(\pi')}{4R} \\
& J_\mu(\pi_{new}) - J_\mu(\pi) \geq \frac{\mathbb{A}^2_{\pi,\mu}(\pi')}{8R}
\end{align}
When the true advantage cannot be computed and an estimate must be used, monotonic improvement can still be guaranteed with high probability. The resulting algorithm is called \ac{CPI}.
This methodology inspired the development of several later safe methods.

\subsection{Unique and Multiple-Parameter Safe Policy Improvement}
Pirotta et al. \cite{pirotta2013safe} derived a more general bound on the performance improvement of a policy $\pi'$ over $\pi$:
\begin{equation}\label{eq:uspi_bound}
J_\mu(\pi_{new}) - J_\mu(\pi) \geq \mathbb{A}_{\pi,\mu}(\pi') - \frac{\gamma}{(1-\gamma)^2}
	\norm[\infty]{\pi'-\pi}^2 \frac{\norm[\infty]{Q^\pi}}{2},
\end{equation}
where the penalty induced by large policy updates is made more evident by the term $\norm[\infty]{\pi'-\pi}^2$. Starting from this bound, a new algorithm called \ac{USPI} was developed. \ac{USPI} uses the same update as \ac{CPI} (\ref{eq:cpi}), but with a step size based on a better improvement bound, derived from \ref{eq:uspi_bound}. This results in better guaranteed performance improvements.
In the same paper, the authors also propose a more general, state dependent policy update:
\[
	\pi_{new}(a \mid s) = (1-\alpha(s))\pi(a \mid s) + \alpha(s)\pi'(a \mid s). 
\]
Using state-dependent step-sizes, it is possible to limit the update to the states in which the average advantage $\expected{a}{\pi'}{A^\pi(s,a)}$ is positive, by setting $\alpha(s) = 0$ for all other states. This is the idea behind \ac{MSPI}. Although more general, this algorithm is not guaranteed to provide better performance improvements than \ac{USPI}, although it is never worse than \ac{CPI}. Moreover, the computation of the optimal step-sizes for 'good' states can only be done iteratively, limiting efficiency.

\subsection{Linearized Policy Improvement}
Bartlett et al. \cite{abbasi2016fast} propose a different policy update employing state-dependent mixture coefficients, which in this case can be computed efficiently.
The policy update is:
\[
	\pi_{new}(a \mid s) = \nu(a \mid s)(1+\alpha\Delta_{\pi}(s,a)),
\]
where:
\begin{itemize}
\item $\nu(\cdot \mid s)$ is a policy such that:
\begin{equation}\label{eq:same_value}
\expected{a}{\pi}{\hat{Q}^{\pi}(s \mid a)} = \expected{a}{\nu}{\hat{Q}^{\pi}(s \mid a)} + 
	\mathop{\mathbf{Var}}\limits_{a \sim \nu}\left[\hat{Q}^{\pi}(s \mid a)\right],
\end{equation}
which can be computed for each visited state with a binary search.
\item $\Delta_\pi(s,a)$ is a normalized action-value function:
\begin{equation}\label{eq:normalized_q}
	\Delta_\pi(s,a) = \hat{Q}^\pi(s,a) - \expected{a}{\nu}{\hat{Q}^\pi(s,a)},
\end{equation}
\item $\alpha$ is a step size.
\end{itemize}
The idea is the following: the original policy $\pi$ is reshaped into:
\[
	\overline{\pi}(a \mid s) = \nu(a \mid s)(1+\Delta_{\pi}(s,a)),
\]
which has the same value function (in the absence of estimation error), i.e.\ $V^{\overline{\pi}} = V^\pi$. The negative term in Equation \ref{eq:normalized_q} ensures that $\overline{\pi}$ is a well defined distribution. Introducing a step size $\alpha$ allows to assign higher probability to actions with positive normalized value function $\Delta_\pi$. A conservative choice for $\alpha$ is $\nicefrac{1}{\norm[\infty]{\Delta_\pi}}$, but the authors also propose more practical alternatives.
The resulting algorithm is called \ac{LPI}. The authors show that the performance improvement guarantee of \ac{LPI} is better than the one of \ac{CPI}, at the same time allowing for bigger policy updates, hence faster convergence.

\section{Safe Policy Gradient}\label{sec:spg}
\subsection{The role of the step size}
Most of the safe policy gradient methods that have been proposed so far focus on the selection of the proper step size $\alpha$.
The step size regulates the magnitude of the parameter updates. Although the relationship between $\Delta\vtheta$ and the actual change in the policy is parametrization-dependent (at least for vanilla policy gradients) and may be non-trivial, in general small step sizes produce small changes in the policy, hence small performance oscillations. However, reducing the step-size also reduces the convergence speed, which is a major drawback in most applications. 
The traditional solution is to employ line search to select the optimal step size for each problem. Although effective in supervised learning \cite{More:1994:LSA:192115.192132}, this approach can be prohibitively expensive in \ac{RL}. 
An alternative is to make the step size adaptive, i.e.\ to compute it at each learning iteration from collected data.

\subsection{Adaptive step-size}\label{sec:adaptive_ss}
Pirotta et al. \cite{NIPS2013_5186} apply the approach of Kakade and Langford \cite{kakade2002approximately} to policy gradient. Starting from a continuous version of Bound \ref{eq:uspi_bound}, they derive a series of bounds on performance improvement which can be maximized w.r.t.\ the step size $\alpha$. We report here a bound for Gaussian policies which does not require to compute the action-value function:
\begin{multline}\label{eq:pirotta_bound2}
J_{\mu}(\vtheta')-J_{\mu}(\vtheta) \geq \alpha\norm[2]{\gradJ{\vtheta}}^2 - \\
	\alpha^2\frac{RM_\phi^2\norm[1]{\gradJ{\vtheta}}^2}{(1-\gamma)^2\sigma^2}
	\left(\frac{|\mathcal{A}|}{\sqrt{2\pi}\sigma} + \frac{\gamma}{2(1-\gamma)}\right),
\end{multline}
where $M_\phi$ is a bound on state features $\phi(s)$.
At each learning iteration, given $\gradJ{\vtheta}$, the step size that optimizes this bound can be computed:
\[
\alpha^* = \frac{\norm[2]{\gradJ{\vtheta}}^2(1-\gamma)^3\sigma^3\sqrt{2\pi}}
	{RM_\phi^2\norm[1]{\gradJ{\vtheta}}^2(2|\mathcal{A}|(1-\gamma)+\sqrt{2\pi}\sigma\gamma)}
\] 
Updating the policy parameters with $\alpha^*$ guarantees a constant performance improvement:
\[
J_{\mu}(\vtheta')-J_{\mu}(\vtheta) \geq
	\frac{\norm[2]{\gradJ{\vtheta}}^4(1-\gamma)^3\sigma^3\sqrt{2\pi}}
	{2RM_\phi^2\norm[1]{\gradJ{\vtheta}}^2(2|\mathcal{A}|(1-\gamma)+\sqrt{2\pi}\sigma\gamma)}
\]
When the policy gradient cannot be computed exactly, monotonicity can be guaranteed with high probability, depending on the entity of the estimation error. The authors provide methods, based on Chebyshev's inequality, to select the number of samples necessary to achieve a desired improvement probability $\delta$, for the cases of REINFORCE and PGT gradient estimators.
\paragraph{} % Lipschitz
The same authors \cite{pirotta2015policy} follow a similar approach to derive tighter bounds for the case of Lipschitz-continuous \ac{MDP}s. A Lipschitz \ac{MDP} is a \ac{MDP} with Lipschitz-continuous transition model $\mathcal{P}$ and reward function $\mathcal{R}$. We omit the mathematical details, but intuitively the Lipschitz hypothesis is reasonable when the dynamics of the environment are sufficiently smooth. Starting from this hypothesis, and constraining the policy to be Lipschitz-continuous as well, the authors are able to prove the Lipschitz continuity of the policy gradient $\gradJ{\vtheta}$ and provide an efficient algorithm to compute an optimal step size based on this fact.
 
\subsection{Beyond the step size}\label{sec:beyond}
Although the step size is crucial in controlling policy oscillation, we saw that also the entity of the estimation error plays a role. In likelihood ratio methods, the variance of the policy gradient estimate $\gradApp{\vtheta}$ can be limited by averaging over a large number of trajectories. In this case, the batch size $N$ can affect policy improvement. In \cite{NIPS2013_5186} the effect of $N$ is recognized but not exploited: it is still a meta-parameter that must be selected beforehand by a human expert. As in the case of the step size, line search algorithm are usually too expensive given the time that is typically required to collect a sample trajectory. For the same reason, an adaptive batch size should take into consideration the cost of collecting more samples, which cannot be neglected in practical applications. In the following chapter we address this problems to derive a cost-sensitive adaptive batch size that guarantees monotonic improvement.



\section{Other Safe Methods}\label{sec:other_safe}
In this section we describe other safe approaches to reinforcement learning.

\subsection{Trust Region Policy Optimization}
A problem with the methods proposed so far is that they tend to be over conservative, negatively affecting convergence speed. Schulman et al. \cite{schulman2015trust} revisit the \ac{CPI} approach to develop an effective, although strongly heuristic, safe direct policy search algorithm. Given a parametrized policy $\pi_{\vtheta}$, said $\vtheta$ the current parameter and $\vtheta'$ a generic updated parameter, the following is shown to hold (adapted to our notation):
\[
	J_\mu(\vtheta) \geq L_{\vtheta}(\vtheta') -\alpha H_{max}(\vtheta,\vtheta'),
\]
where $L_{\vtheta}(\vtheta') = J_\mu(\vtheta) + \mathop{\mathbf{E}}\limits_{s \sim d_{\mu}^{\pi_{\vtheta}} , a \sim \pi_{\vtheta'}}\left[A_{\pi_{\vtheta}}(s,a)\right]$ is a first order approximation of the performance improvement, $\alpha=\frac{2\max_{s \in \mathcal{S}}{\expected{a}{\pi_{\vtheta'}}{A^{\pi}(s,a)}}}{(1-\gamma)^2}$ is a penalty coefficient and $H_{max}(\vtheta,\vtheta')=\max_{s \in \mathcal{S}}{H(\pi_{\vtheta},\pi{\vtheta})}$, where $H(\cdot,\cdot)$ is the Kullback-Liebler divergence between two distributions.
The traditional approach would be to maximize the bound:
\[
	\vtheta' = \arg\max\limits_{\tilde{\vtheta} \in \Theta}{L_{\vtheta}(\tilde{\vtheta}) -\alpha H_{max}(\vtheta,\tilde{\vtheta})},
\]
but the theoretically justified penalty coefficient $\alpha$ leads to updates that are too conservative. Instead, the authors propose to resolve to the following constrained optimization problem:
\begin{align}
\vtheta' = &\arg\max\limits_{\tilde{\vtheta} \in \Theta}{L_{\vtheta}(\tilde{\vtheta})} \\
	&\text{subject to } H_{max}(\vtheta,\tilde{\vtheta}) \leq \delta. \label{eq:tr_constraint}
\end{align}
Constraint \ref{eq:tr_constraint} is called trust region constraint and its intuitive role is to limit the update to a neighborhood of the current parameter $\vtheta$ to avoid oscillation. A series of approximations to this exact method lead to the \ac{TRPO} algorithm, which shows to be effective on complex tasks.

\subsection{High Confidence Policy Improvement}
The methods seen so far try to achieve monotonic improvement. Thomas et al. \cite{thomas2015high} adopt a looser definition of safety: an algorithm is safe if it always returns a policy with performance below $J_{\mu}^-$ with probability at most $\delta$, where both $J_{\mu}^-$ and $\delta$ are selected by the user. In this way, different applications allow for different safety requirements, depending on risk aversion. The authors propose a batch \ac{RL} algorithm: the set of sample trajectories is partitioned into a training set and a test set. The training set is searched for the policy that is predicted to achieve the best performance among all the policies that are predicted to be safe. The safety of the candidate policy is then tested on the test set. The authors also describe an episodic variant of this algorithm, called DAEDALUS. If compared with other safe approaches, these algorithms require less data and have no meta-parameters requiring expert tuning, but have a higher computational complexity and do not guarantee monotonic improvement. 

\subsection{Robust Baseline Regret Minimization}
Petrik et al. \cite{petrik2016safe} propose a model-based batch algorithm that guarantees improvement over a baseline policy $\pi$.
Given a model of the environment $\hat{P}$ and estimation error $e$, uncertainty set $\Xi(\hat{P},e)$ is the set of transition dynamics within $e$.  The new policy is selected by solving the following adversarial optimization problem:
\begin{equation}\label{eq:petrik}
\pi' \in \arg\max\limits_{\tilde{\pi}}\min_{\xi \in \Xi}
	\left(J_{\mu}(\tilde{\pi}\mid \xi)- J_{\mu}(\pi\mid \xi)\right), 
\end{equation}
which selects the best policy given the worst possible dynamics. Solutions to this problem are guaranteed to be safe. Unfortunately, solving \ref{eq:petrik} is NP-hard. The authors provide an approximate algorithm (Approximate Robust Baseline Regret Minimization) with promising results.