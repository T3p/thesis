\chapter{State of the Art}

In this chapter we provide an overview of the most commonly used policy gradient methods, with a focus on safe approaches.


\section{Policy Gradient in General}
In this section we present common policy gradient algorithms. The aim is not to provide an exhaustive survey of policy gradient methods, but to give a proper context to the main topic of this and the following chapter: safe policy gradients.

\subsection{Why policy gradient}\label{sec:why_pg}
Traditional \ac{RL} methods rely on the computation (or the estimation) of the action-value function $Q^{\pi}$. These so-called value function methods were first proposed for solving simple \ac{MDP}s characterized by small, discrete state and action spaces. They are also known as tabular methods, since the value function can be stored in a finite table. In this limited scenario, tabular algorithms have strong convergence guarantees and perform well in practice. However, they become unfeasible when the number of possible states and actions is too big or even infinite, such as in continuous \ac{MDP}s.
The natural extension of tabular methods is to employ the function approximation tools known from supervised learning, such as neural networks, to represent $Q^{\pi}$. However, this approach have many problems, especially when applied to control tasks:
\begin{itemize}
\item Value function approximation methods have no guarantee of convergence, not even to locally optimal policies. In some applications, divergence may even damage the system.
\item Greedy policies based on approximated value functions can be highly sensitive to small perturbations in the state, being unfeasible for tasks characterized by noise, e.g.\ control tasks with noisy sensors.
\item Given the complexity of the value function representation (think of a multi-layer perceptron), it is difficult to isolate potentially dangerous behaviors.
\end{itemize}
Policy gradient methods bypass these problems by searching directly for the optimal policy. Convergence to a local optimum is guaranteed (also for stochastic methods), uncertainty in the state have controllable effects and policies can be made safe by design. Moreover, prior domain knowledge can be exploited to design policies suited for the specific tasks.

\paragraph{} %Application examples
Indeed, policy gradient methods have been successfully applied to complex control tasks. In \cite{Sehnke2008policy} the authors were able to solve a robot standing task, where a (simulated) biped robot must keep an erect position while perturbed by external forces. In \cite{kober_NIPS2008} Kober and Peters used policy search in combination with imitation learning to teach the Ball-in-a-Cup game to a real robotic arm. A similar approach was used in \cite{Peters2008natural} for a baseball swing task. In \cite{peters2010relative} policy search was used to play simulated robot table tennis. 

\subsection{From finite differences to likelihood ratio}
Among the oldest policy gradient methods we find finite-difference methods, which arose in the stochastic simulation literature. The idea is to perturb the policy parameter $\vtheta$ many times, obtaining $\vtheta+\Delta\vtheta_1 \dots \vtheta+\Delta\vtheta_K$, and estimate for each perturbation the expected return difference $\Delta\hat{J}_i \simeq J(\vtheta+\Delta\vtheta_i) - J(\vtheta)$, by performing a number of sample trajectories. After collecting all these data, the policy gradient can be estimated by regression as:
\[
	\gradApp{\vtheta} = (\boldsymbol{\Delta\Theta}^T\boldsymbol{\Delta\Theta})^{-1}\boldsymbol{\Delta\Theta}^T\boldsymbol{\Delta\hat{J}}
\]
where $\boldsymbol{\Delta\Theta}^T = [\Delta\vtheta_1,\dots,\Delta\vtheta_K]^T$ and $\boldsymbol{\Delta\hat{J}} = [\Delta\hat{J}_1,\dots,\Delta\hat{J}_K]^T$. This method is easy to implement and very efficient when applied to deterministic tasks or pseudo-random number simulations. In real control tasks though, perturbing the policy parametrization without incurring in instability may not be trivial and noise can have a disastrous impact on performance. Moreover, gradient estimation requires a large number of trajectories.
\paragraph{} %Likelihood methods
The likelihood ratio approach allows to estimate the gradient even from a single trajectory, without the need of perturbing the parametrization. This method is due to Williams \cite{Williams1992}, but is better understood from the perspective of the Policy Gradient Theorem \cite{Sutton1999a}.
By applying trivial differentiation rules to the expression of the policy gradient (see Theorem \ref{theo:pgt}), we have:
\[
	\gradJ{\vtheta} = \int_{\mathcal{S}}d_{\mu}^{\pi}(s)
		\int_{\mathcal{A}}\pi_{\vtheta}(a \mid s)\score
		Q^{\pi}(s,a)\mathrm{d}a\mathrm{d}s.
\]
This is known as the REINFORCE trick. The term
\[
	\score = \frac{\nabla_{\vtheta}\pi_{\vtheta}(s,a)}{\pi_{\vtheta}(s,a)}
\]
has many names: eligibility vector, characteristic eligibility, policy score and, of course, likelihood ratio.
In terms of the joint distribution $\zeta$, the policy gradient is just:
\[
	\gradJ{\vtheta} = \mathop{\mathbf{E}}_{(s,a)\sim\zeta}\left[\score Q^{\pi}(s,a)\right].
\]
Since sampling state-action pairs from $\zeta$ is equivalent to following policy $\pi_{\vtheta}$, the gradient can be estimated from a single trajectory as:
\[
	\gradSim{\vtheta} = \langle\score Q^{\pi}(s,a)\rangle_H,
\]
where $\langle\cdot\rangle^H$ denotes sample average over $H$ time steps. Of course a more stable estimate can be obtained by averaging again over a batch of $N$ trajectories:
\[
	\gradApp{\vtheta} = \langle\langle\score Q^{\pi}(s,a)\rangle_H\rangle_N.
\]

\subsection{Baselines}
A problem of the REINFORCE approach is the large variance of the estimate, which results in slower convergence.
The Policy Gradient Theorem still holds when an arbitrary baseline $b(s)$ is subtracted from the action-value function, as shown in \cite{Williams1992}:
\[
	\gradJ{\vtheta} = \mathop{\mathbf{E}}_{(s,a)\sim\zeta}
		\left[\score (Q^{\pi}(s,a) - b(s))\right].
\]
The baseline can be chosen to reduce the variance of the gradient estimate without introducing any bias, speeding up convergence. A natural choice of baseline is the state-value function $V^{\pi}$:
\begin{align*}
	\gradJ{\vtheta} &= \mathop{\mathbf{E}}_{(s,a)\sim\zeta}
		\left[\score (Q^{\pi}(s,a) - V^{\pi}(s))\right] \\
		&= \mathop{\mathbf{E}}_{(s,a)\sim\zeta}
				\left[\score (A^{\pi}(s,a))\right].
\end{align*}
The usage of the advantage function with the eligibility vector has an intuitive justification: to follow the policy gradient direction means to assign to the most advantageous actions the highest probability of being taken. 

\subsection{The REINFORCE algorithm}
In practical tasks, the exact value function $Q^{\pi}$ is not available and must be estimated. Episodic likelihood ratio algorithms estimate it from the total return at the end of the episode, assigning to each visited state-action pair a value in retrospective. The REINFORCE gradient estimator \cite{Williams1992} uses the total return directly:
\[
	\hat{\nabla}_{\vtheta}J_{\mu}^{RF}(\vtheta) = 
		\left\langle
		\sum\limits_{k=1}^H\nabla_{\vtheta}\log\pi_{\vtheta}(a_k^n \mid s_k^n)
		\left(\sum\limits_{l=1}^H\gamma^{l-1}r_l^n-b\right)\right\rangle_N.
\]
A problem of the REINFORCE algorithm is the high variance of the gradient estimate, which can be reduced by using a proper baseline. A variance-minimizing baseline can be found in \cite{Peters2008reinf}:
\begin{align*}
	b &= \frac{\left\langle\left(\sum\limits_{k=1}^H\nabla_{\vtheta}\log\pi_{\vtheta}(a_k^n \mid s_k^n)\right)^2
		\sum\limits_{l=1}^H\gamma^{l-1}r_l^n\right\rangle_N}
		{\left\langle\left(\sum\limits_{k=1}^H\nabla_{\vtheta}\log\pi_{\vtheta}(a_k^n \mid s_k^n)\right)^2
		\right\rangle_N}.
\end{align*}
Note that, in the case $|\vtheta|>1$, all operations among vectors are to be intended per-component or, equivalently, each gradient component $\gradApp{\vtheta_i}$ must be computed separately.

\subsection{The PGT/G(PO)MDP algorithm}
A problem of the REINFORCE gradient estimator is that the value estimate for $(s_k^n,a_k^n)$ depends on past rewards. Two refinements are the PGT gradient estimator \cite{Sutton2000policy}:
\[
	\gradPGT{\vtheta} = 
		\left\langle
		\sum\limits_{k=1}^H\nabla_{\vtheta}\log\pi_{\vtheta}(a_k^n \mid s_k^n)
		\left(\sum\limits_{l=k}^H\gamma^{l-1}r_l^n-b\right)\right\rangle_N,
\]
and the G(PO)MDP gradient estimator \cite{Baxter2001infinite}:
\[
	\gradGPOMDP{\vtheta} = 
			\left\langle
			\sum\limits_{l=1}^H\left(\sum\limits_{k=1}^l
			\nabla_{\vtheta}\log\pi_{\vtheta}(a_k^n \mid s_k^n)\right)
			(\gamma^{l-1}r_l^n-b)\right\rangle_N.
\]
Although the algorithms are different, the gradient estimator that is computed is exactly the same, i.e.\ $\gradPGT{\vtheta} = \gradGPOMDP{\vtheta}$, as shown in \cite{Peters2008reinf}. From now on we will refer to the PGT form.
In the case $b=0$, the PGT gradient estimation has been proven to suffer from less variance than REINFORCE \cite{Zhao2011a} under mild assumptions. A variance-minimizing baseline for PGT is provided in \cite{Peters2008reinf}. Differently from the REINFORCE case, a separate baseline $b_l$ is used for each time step $l$:
\begin{align*}
	b_l &= \frac{\left\langle\left(\sum\limits_{k=1}^l\nabla_{\vtheta}\log\pi_{\vtheta}(a_k^n \mid s_k^n)\right)^2
		\gamma^{l-1}r_l^n\right\rangle_N}
		{\left\langle\left(\sum\limits_{k=1}^l\nabla_{\vtheta}\log\pi_{\vtheta}(a_k^n \mid s_k^n)\right)^2
		\right\rangle_N} \\\\
	\gradPGT{\vtheta} &= 
		\left\langle
		\sum\limits_{k=1}^H\nabla_{\vtheta}\log\pi_{\vtheta}(a_k^n \mid s_k^n)			\left(\sum\limits_{l=k}^H\gamma^{l-1}r_l^n-b_l\right)\right\rangle_N	
\end{align*}


\subsection{Natural gradients}
The algorithms described so far use the so-called vanilla policy gradient, i.e.\ an unbiased estimate of the true gradient w.r.t.\ policy parameters.
Since such algorithms perform a search in parameter space $\Theta$, the performance is strongly depends on the policy parametrization.
Natural policy gradients allow to search directly in policy space, independently on the parametrization. The natural policy gradient update rule is \cite{Kakade:2001}:
\[
	\vtheta \gets \vtheta + F_{\vtheta}\gradJ{\vtheta},
\]
where $F_{\vtheta}$ is the Fisher information matrix:
\begin{align*}
	F_{\vtheta} &= \int_{\mathcal{S}}d_{\mu}^{\pi}(s)
			\int_{\mathcal{A}}\pi_{\vtheta}(a \mid s)\score
			\score^T\mathrm{d}a\mathrm{d}s \\
			&\simeq \langle\score\score^T\rangle_H.
\end{align*}
The parameter update is in the direction of a rotated policy gradient. Since the angular difference is never more than $\nicefrac{\pi}{2}$ \cite{Amari1998natural}, convergence to a local optimum is still guaranteed. 
Although a good policy parametrization can yield better performance for specific problems, natural policy gradients offer a more general approach and have shown effective in practice \cite{Peters2008natural}.

\subsection{Actor-critic methods}
Actor-critic methods try to combine policy search and value function estimation to perform low-variance gradient updates. The "actor" is a parametrized policy $\pi_{\vtheta}$, which can be updated as in vanilla policy gradient algorithms. For this reason, the policy gradient methods described so far are also called critic-only. The "critic" is an estimate of the action-value function $\hat{Q}_{\boldsymbol{w}}$, where $\boldsymbol{w}$ is a parameter vector. The approximate value-function methods briefly mentioned in Section \ref{sec:why_pg} can be thought as critic-only. The \ac{RL} literature provides a great number of methods to learn a value function estimate from experience \cite{Sutton:1998:IRL:551283}. We provide an example of episodic value-function parameter update:
\[
	\boldsymbol{w} \gets \boldsymbol{w} + \beta\langle
		(R_t-\hat{Q}_{\boldsymbol{w}}(s_t,a_t))
		\nabla_{\boldsymbol{w}}\hat{Q}_{\boldsymbol{w}}(s_t,a_t)\rangle_H,
\] 
where $R_t = \sum\limits_{k=0}^{\infty}\gamma^kr_{t+k}$ and $\beta$ is just a learning rate.
Actor and critic meet when $\hat{Q}_{\boldsymbol{w}}$ is used to compute the policy gradient used to update $\vtheta$:
\begin{equation}\label{eq:ac_update} 
	\vtheta \gets \vtheta + \langle\score \hat{Q}_{\boldsymbol{w}}(s,a)\rangle_H.
\end{equation}
This approach exploits the predictive power of approximate value functions while preserving the nice theoretical properties of the policy gradient. Under some assumption on the critic function (see Theorem 2 in \cite{Sutton1999a}), the Policy Gradient Theorem is still valid when $\hat{Q}_{\boldsymbol{w}}$ is used in place of the true value function $Q^\pi$:
\[
	\gradJ{\vtheta} = \int_{\mathcal{S}}d_{\mu}^{\pi}(s)
		\int_{\mathcal{A}}\pi_{\vtheta}(a \mid s)\score
		\hat{Q}_{\boldsymbol{w}}(s,a)\mathrm{d}a\mathrm{d}s.
\]
This means that parameter update (\ref{eq:ac_update}) is still in the direction of the policy gradient, guaranteeing convergence to a local optimum.
For a recent survey of actor-critic methods refer to \cite{grondman2012survey}, which reports also examples of the usage of natural gradients in combination with the actor-critic approach.

\section{Safe policy gradient methods}
In this section we present the state of the art on monotonically improving policy gradient methods, which are also known as conservative or safe approaches. In the following we will refer to them as safe policy gradient methods.

\subsection{The need for safe policy gradients}
Exploration is a key component of \ac{RL} algorithms. Through exploration, new states are discovered and new actions are tried and evaluated, increasing the agent's knowledge of the environment. The information gain, though, is paid in terms of performance, since gathering new knowledge takes precious time that could be spent in performing a good policy, based on the knowledge that has already been acquired. This is known as the exploration-exploitation dilemma. In policy gradient methods, exploration can be regulated at two levels: the intrinsic stochasticity of the policy (e.g. by changing the standard deviation of a Gaussian policy) and the parameter updates. We will focus on the latter, although the two problems are not entirely independent. 
\paragraph{}In terms of the policy performance $J_\mu(\vtheta)$, exploratory updates can produce oscillations. If we could plot the exact value of $J_\mu(\vtheta)$ over learning iterations, the policy gradient methods described in the previous subsections would show positive-trend zig-zag progressions. When learning is performed on a simulated task, it is natural to favor exploration, since all that counts is how good the final policy is. This is no longer true when learning is performed on a real system, like in the scenarios presented in the introduction. In this case, uncontrolled exploratory behavior can cause one or both of the following issues:
\begin{itemize}
\item Dangerous behavior: low performance peaks may correspond to large deviations from the initial policy. Even if the initial policy has been designed to be safe, such deviations may result in behavior able to damage the system.
\item Poor long-term performance: when learning is performed simultaneously with production, e.g.\ to automatically adjust an existing optimal policy to small changes in the environment, oscillations in the per-episode performance are paid in term of long-term performance when they are not justified by a relevant enough policy improvement.
\item Selection of suboptimal policies: If the learning process has a fixed duration or must be stopped for some reason, oscillations may lead to the selection of a policy that is not the best seen so far, or is even worse than the initial policy. In highly stochastic environments it can be very difficult to manually reject such sub-par policies.
\end{itemize} 
From these problems comes the necessity of policy updates that are monotonically increasing, i.e.\ safe policy gradients.

\subsection{The step-size problem}
Most of the safe policy gradient methods that have been proposed focus on the selection of the proper step size $\alpha$.
The step size regulates the magnitude of the parameter updates. Although the relationship between $\Delta\vtheta$ and the actual change in the policy is parametrization-dependent (at least for vanilla policy gradients) and may be non-trivial, in general small step sizes produce small changes in the policy, hence small performance oscillations. However, reducing the step-size also reduces the convergence speed, which is a major drawback in most applications. To face this trade-off, the step size can be made adaptive, e.g.\ by making it a function of the policy gradient. Many of the methods described in the remaining part of this section follow this approach, which is also the starting point of our main contribution.

\subsection{State of the art}
\subsection{What to do next}