\chapter{Introduction}\label{chap:intro}
\ac{RL} is a field of machine learning that aims at building intelligent machines capable of learning complex tasks from experience. Among the current challenges that Reinforcement Learning has to face, there are the inner difficulty of learning in continuous domains and the problem of safe learning. The aim of this thesis is to develop a novel approach to safe learning in continuous domains, in the form of an adaptive policy gradient algorithm.

Continuous domains are common in automatic control and robotic applications. Traditional \ac{RL} algorithms, based on value functions, suffer from many problems when applied to such continuous tasks, the most serious being the lack of strong theoretical guarantees.
Policy gradient methods, besides guaranteeing convergence to locally optimal policies \cite{Sutton1999a}, are able to address many of the difficulties that characterize complex control problems, such as high dimensional state spaces and noisy sensors. Moreover, they allow to easily incorporate prior domain knowledge in order to design safer and more effective policies.
Many different policy gradient algorithms have been proposed, all sharing the same basic principle of computing the gradient of a performance measure to update the agent's policy in a direction of improvement. These methods have been successfully applied to complex control tasks (see \cite{deisenroth2013survey} for a recent survey). 
Policy gradient is often used to improve an existing policy, which may be designed by a human expert or learned by simulation. Improvement on the real system can account for the lack of accuracy that necessarily characterizes any model of the environment. Moreover, when the latter is non-stationary, self improvement can make an agent more autonomous, by making it able to adapt to the changes that may occur, without the need of human intervention. We will address the problem of safe learning with this scenario in mind.

During the learning process, it is common to observe oscillations in the agent's performance. In approaching the optimal policy, not all the updates, if taken individually, yield a performance improvement. Although often what counts is the final result, this kind of behavior is unacceptable for many relevant applications. Safe approaches, also known as conservative, aim at guaranteeing monotonic performance improvement during the learning process. There are many reasons for which this property may be desirable, especially in the scenario of our interest. First of all, when learning happens on a real system, time is limited. This means that the promise of a good asymptotic performance is no longer enough, and also partial results become important. Steady improvement is even more fundamental when learning is performed simultaneously with production, for instance to adapt to small changes in the environment in a real-time fashion. In this case, worsening updates may directly affect the performance.
Finally, particularly bad updates may result in dangerous behaviors able to damage the system. 

Safe approaches to \ac{RL} come from the approximate policy iteration literature, starting from the seminal work by Kakade and Langford \cite{kakade2002approximately}. On the basis of their approach, many safe methods have been proposed in the recent years, including safe policy gradient methods. So far, research in this direction has focused on the selection of the step size, also known as learning rate, a meta-parameter that is usually associated with convergence speed, but can also be responsible of performance oscillation. Pirotta et al. \cite{pirotta2015policy} devised an adaptive step size that can be used to guarantee monotonic improvement with high probability. 
Another parameter with similar properties is the batch size, the number of sample trajectories used to estimate the policy gradient. However, to the best of our knowledge, the automatic selection of the batch size has not been taken into consideration so far in the safe \ac{RL} literature.
Large values of the batch size produce more accurate estimates, which result in a steadier improvement. On the other hand, employing a large number of samples is costly, especially in real control applications. Moreover, the selection of the step size is dependent from the batch size that is employed. For all these reasons, the next natural step in safe policy gradient methods is an algorithm where also the batch size is adaptive.  

The aim of this work is to develop an adaptive policy gradient algorithm able to automatically select both meta-parameters, the step size and the batch size. Although the focus is on safety, convergence speed can never be neglected in a practical \ac{RL} algorithm. This means that our algorithm must guarantee monotonic improvement with high probability, but also favor fast convergence when possible. The starting point is represented by the results on adaptive step size for Gaussian policies provided in \cite{pirotta2015policy}. We follow fundamentally the same approach, consisting in maximizing a pessimistic lower bound on the performance improvement. The existing results are first generalized, obtaining an even better performance improvement guarantee. The new formulation is then used to include also the batch size in the optimization process. Since the batch size affects convergence speed only in an indirect manner, a new cost function must be defined that takes into account the cost of collecting samples. Theoretical results point out an interesting relationship between the two meta-parameters, and the resulting algorithm is indeed able to guarantee monotonic improvement. Since the method is highly conservative, we propose some empirical variants that allow to keep the selected batch size under reasonable values. The proposed methods are tested on simple, simulated control task and compared with traditional approaches.

The structure of the remaining part of this document is as follows: Chapter 2 provides preliminary knowledge on Markov Decision Processes and Reinforcement Learning algorithms, with a focus on policy gradient methods. Chapter 3 describes the state of the art of safe approaches to Reinforcement Learning, including safe policy gradient. Chapter 4 contains all the novel theoretical results on the adaptive batch size and presents the new algorithm with all its variants. Chapter 5 discusses the results of numerical simulations on simple control tasks. Chapter 6 draws conclusions on this work and discusses possible future developments.
Appendix A provides all the proofs that are omitted in Chapter 4. Finally, Appendix B represents a first attempt at extending the results to other classes of policies.
