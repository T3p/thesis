\chapter{Extension to Softmax Policies}\label{app:softmax}

In this appendix we show a first attempt at adapting the results on monotonic improvement to the class of Softmax Policies, as suggested in Chapter 6. The impractical nature of this partial results is indicative of the difficulties that characterize this task and could be useful to guide future research on the matter.
\paragraph{}
We fist need some Lemmas about specific properties of Softmax policies:

\begin{lemma}\label{lem:softmax_d2}
If $\pi_{\vtheta}$ is a softmax policy:
\[
\left| \frac{\partial^2\pi_{\vtheta}(a \mid s)}
	{\partial\theta_i\partial\theta_j}\right| \leq
	6M_{\phi}^2
\]
\end{lemma}
\begin{proof}
\begin{align*}
\left|\frac{\partial^2\pi_{\vtheta}(a \mid s)}
		{\partial\theta_i\partial\theta_j}\right|
		&= \left| \pi_{\vtheta}(a\mid s)\left(
		\phi_j(s,\cdot)\phi_i(s,\cdot) -  \phi_i(s,\cdot)\expected{a'}{\pi_{\vtheta}}{\phi_j(s,\cdot)} - \phi_j(s,\cdot)\expected{a'}{\pi_{\vtheta}}{\phi_i(s,\cdot)}\right.\right. \\
		&\left.\left.+ 2\expected{a'}{\pi_{\vtheta}}{\phi_i(s,\cdot)}\expected{a'}{\pi_{\vtheta}}{\phi_j(s,\cdot)} - \expected{a'}{\pi_{\vtheta}}{\phi_i(s,\cdot)\phi_j(s,\cdot)}\right)\right| \\
		\leq 6M_\phi^2
\end{align*}
\end{proof}

\begin{lemma}\label{lem:softmax_diff}
If $\pi_{\vtheta}$ is a softmax policy and $\vtheta' = \vtheta+\Lambda\gradJ{\vtheta}$:
\[
\pi_{\vtheta'}(a \mid s) - \pi_{\vtheta}(a \mid s) \geq 
	\nabla_{\vtheta}\pi_{\vtheta}(a \mid s)^T\Lambda\gradJ{\vtheta} -
	6M_{\phi}^2\norm[1]{\Lambda\gradJ{\vtheta}}^2
\]
\end{lemma}
\begin{proof}
It follows from the application of Lemma \ref{lem:softmax_d2} to Taylor's expansion.
\end{proof}

\begin{lemma}\label{lem:softmax_infnorm}
If $\pi_{\vtheta}$ is a softmax policy and $\vtheta' = \vtheta+\Lambda\gradJ{\vtheta}$:
\[
	\norm[\infty]{\pi_{\vtheta'} - \pi_{\vtheta}}^2 \leq 4M_\phi\norm[1]{\Lambda\gradJ{\vtheta}}
\]
\end{lemma}
\begin{proof}
By Pinsker's inequality:
\begin{align*}
\norm[\infty]{\pi_{\vtheta'} - \pi_{\vtheta}}^2 = \sup_s\norm[1]{\pi_{\vtheta'} - \pi_{\vtheta}}^2 \leq \sup_s\left(2H(\pi_{\vtheta'}\mid\mid\pi_{\vtheta})\right),
\end{align*}
where $H(\cdot\mid\mid\cdot)$ is the Kullback-Liebler divergence, which in this case can be bounded as:
\begin{align*}
H(\pi_{\vtheta'},\pi_{\vtheta}) &= 
\sum_{a \in \mathcal{A}}
	\frac{e^{\vtheta'^T\phi(s,a)}}{\sum_{a'\in\mathcal{A}}e^{\vtheta'^T\phi(s,a')}}
	\log\frac{e^{\vtheta'^T\phi(s,a)}\sum_{a'\in\mathcal{A}}e^{\vtheta^T\phi(s,a')}}
	{e^{\vtheta^T\phi(s,a)}\sum_{a'\in\mathcal{A}}e^{\vtheta'^T\phi(s,a')}} \\
	&= \expected{a}{\pi_{\vtheta'}}{
	\Delta\vtheta^T\phi(s,a) + \log\frac{\sum_{a'\in\mathcal{A}}e^{\vtheta^T\phi(s,a')}}
	{\sum_{a'\in\mathcal{A}}e^{\vtheta'^T\phi(s,a')}}} \\
	&= \expected{a}{\pi_{\vtheta'}}{
	\Delta\vtheta^T\phi(s,a) + \log\frac{\sum_{a'\in\mathcal{A}}e^{\vtheta^T\phi(s,a')}}		{\sum_{a'\in\mathcal{A}}e^{\vtheta^T\phi(s,a')}e^{\Delta\vtheta^T\phi(s,a')}}} \\
	&\leq \expected{a}{\pi_{\vtheta'}}{
	\norm[1]{\Delta\vtheta}M_\phi + \log\frac{\cancel{\sum_{a'\in\mathcal{A}}e^{\vtheta^T\phi(s,a')}}}
	{e^{-\norm[1]{\Delta\vtheta}M_\phi}\cancel{\sum_{a'\in\mathcal{A}}e^{\vtheta^T\phi(s,a')}}}} \\
	&= 2\norm[1]{\Delta\vtheta}M_\phi = 2M_\phi\norm[1]{\Lambda\gradJ{\vtheta}} 	
\end{align*}
The rest of the proof is trivial.
\end{proof}

We can now state the following theorem, which is a variant of Theorem \ref{theo:1}

\begin{theorem}\label{theo:softmax1}
For any initial state distribution $\mu$, if $\pi_{\vtheta}$ is a softmax policy and $\vtheta' = \vtheta+\Lambda\gradJ{\vtheta}$:
\begin{align*}
\DeltaJ &\geq \gradJ{\vtheta}^T\Lambda\gradJ{\vtheta} \\
	&-c\norm[1]{\Lambda\gradJ{\vtheta}}^2 -
	b\norm[1]{\Lambda\gradJ{\vtheta}},
\end{align*}
where
\begin{align*}
c &= \frac{6M_{\phi}^2|\mathcal{A}|R}{(1-\gamma)^2}, \\
b &=  \frac{2\gamma M_{\phi}R}{(1-\gamma)^3}
\end{align*}
\end{theorem}
\begin{proof}
It's enough to plug the results from Lemmas \ref{lem:softmax_diff} and \ref{lem:softmax_d2} into Lemma 3.1 from \cite{NIPS2013_5186}.
\end{proof}

\begin{corollary}\label{cor:softmax_stepsize}
The performance lower bound of Theorem \ref{theo:softmax1} is maximized, under the assumption that $b\leq \norm[\infty]{\gradJ{\vtheta}}$, by the following non-scalar step size:
\[ \alpha_{k}^*=	
\begin{cases}
	\frac{1}
		{2c}\left(1-\frac{b}{\norm[\infty]{\gradJ{\vtheta}}}\right) & 
		\text{if } k = \arg\max\limits_i |\gradJ{\theta_i}|,	\\
		0 & \text{otherwise},
\end{cases}
\]
which guarantees the following performance improvement: 
\[
\DeltaJ \geq \frac{\left(\norm[\infty]{\gradJ{\vtheta}} - b\right)^2}{4c}.
\]
\end{corollary}
\begin{proof}
The proof is very similar to the one of Corollary \ref{cor:1}, so we omit it.
\end{proof}

The issue with the result of Corollary \ref{cor:softmax_stepsize} is that, in practical applications, the constant $b$ can easily be larger than $\norm[\infty]{\gradJ{\vtheta}}$. This means that, in many cases, no improvement-guaranteeing step size can be found, not even when the policy gradient is known exactly.
Although this result cannot be used in practice, it shows that the main difficulty in extending our method to Softmax policies is the characterization of the policy difference measure $\norm[\infty]{\pi_{\vtheta'} - \pi_{\vtheta}}^2$. Further research should be aimed at overcoming or bypassing this issue.