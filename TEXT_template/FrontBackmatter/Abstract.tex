%*******************************************************
% Abstract
%*******************************************************
%\renewcommand{\abstractname}{Abstract}
\addcontentsline{toc}{chapter}{\abstractname}

\pdfbookmark[1]{Abstract}{Abstract}
\begingroup
\let\clearpage\relax
\let\cleardoublepage\relax
\let\cleardoublepage\relax

\chapter*{Abstract}
Policy gradient methods are among the best Reinforcement Learning techniques to solve complex control problems. The application of these methods to real systems call for algorithms capable of guaranteeing a constant improvement of the policies that are tried during the learning process, known as safe algorithms. The research on safe policy gradient methods has so far focused on the selection of the step size of policy updates. Another important parameter is the batch size, that is the number of samples used to estimate the gradient direction for each update, but it has not received the same attention so far. In this thesis we propose a method to jointly optimize the step size and the batch size to achieve (with high probability) monotonic improvement. Theoretical guarantees are accompanied by numerical simulations to analyze the behavior of the proposed algorithms.  

\vfill
\newpage
\pdfbookmark[1]{Sommario}{Sommario}
\chapter*{Sommario}
I metodi 'policy gradient' ('gradiente della politica') rappresentano una delle tecniche più efficaci per risolvere problemi di controllo complessi. L'applicazione di questi metodi a sistemi reali rende necessario lo sviluppo di algoritmi cosiddetti 'safe', traducibile con 'prudenti', capaci di garantire un miglioramento costante delle politiche che vengono provate nel corso dell'apprendimento. La ricerca su metodi policy gradient safe si è concentrata, finora, sulla scelta della 'step size', che regola l'entità degli aggiornamenti della politica. Un altro parametro importante è la 'batch size', il numero di campioni usati per stimare la direzione del gradiente per ogni aggiornamento, che finora, tuttavia, non ha ricevuto pari attenzioni. In questa tesi viene proposto un metodo per ottimizzare congiuntamente entrambi i parametri per ottenere (con probabilità elevata) un miglioramento costante della politica. I risultati teorici sono accompagnati da simulazioni numeriche volte ad analizzare il comportamento degli algoritmi proposti.

\endgroup